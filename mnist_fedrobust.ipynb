{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import tensorflow as tf\n",
    "import tflib as lib\n",
    "import tflib.plot\n",
    "import tflib.mnist_fed\n",
    "import tflib.sn as sn\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net = \"AlexNet\" #Options: AlexNet,Inception,ResNet\n",
    "BATCH_SIZE = 50 # Batch size\n",
    "TEST_BATCH_SIZE = 1000\n",
    "ITERS = 10000 # How many generator iterations to train for \n",
    "INPUT_DIM = 784 # Number of pixels in MNIST (28*28)\n",
    "nodes = 100\n",
    "maximize_iters = 1\n",
    "test_iters = 100\n",
    "noise_std = 0.01\n",
    "\n",
    "address = 'mnist_'+neural_net\n",
    "\n",
    "if not os.path.exists(address):\n",
    "    os.makedirs(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def incept(input_x, input_filters, ch1_filters, ch3_filters, spectral_norm=True, tighter_sn=True,\n",
    "           scope_name='incept', update_collection=None, beta=1., bn=True, reuse=None, training=False):\n",
    "    \"\"\"Inception module\"\"\"\n",
    "        \n",
    "    with tf.variable_scope(scope_name, reuse=reuse):\n",
    "        ch1_output = tf.nn.relu(sn.conv2d(input_x, [1, 1, input_filters, ch1_filters],\n",
    "                                          scope_name='conv_ch1', spectral_norm=spectral_norm,\n",
    "                                          xavier=True, bn=bn, beta=beta, tighter_sn=tighter_sn,\n",
    "                                          update_collection=update_collection, reuse=reuse, training=training))\n",
    "        ch3_output = tf.nn.relu(sn.conv2d(input_x, [3, 3, input_filters, ch3_filters],\n",
    "                                          scope_name='conv_ch3', spectral_norm=spectral_norm,\n",
    "                                          xavier=True, bn=bn, beta=beta, tighter_sn=tighter_sn,\n",
    "                                          update_collection=update_collection, reuse=reuse, training=training))\n",
    "        return tf.concat([ch1_output, ch3_output], axis=-1)\n",
    "\n",
    "\n",
    "def downsample(input_x, input_filters, ch3_filters, spectral_norm=True, tighter_sn=True,\n",
    "               scope_name='downsamp', update_collection=None, beta=1., bn=True, reuse=None, training=False):\n",
    "    \"\"\"Downsample module\"\"\"\n",
    "        \n",
    "    with tf.variable_scope(scope_name, reuse=reuse):\n",
    "        ch3_output = tf.nn.relu(sn.conv2d(input_x, [3, 3, input_filters, ch3_filters], tighter_sn=tighter_sn,\n",
    "                                          scope_name='conv_ch3', spectral_norm=spectral_norm,\n",
    "                                          xavier=True, bn=bn, stride=2, beta=beta, reuse=reuse,\n",
    "                                          update_collection=update_collection, training=training))\n",
    "        pool_output = tf.nn.max_pool(input_x, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                                     padding='SAME', name='pool')\n",
    "        return tf.concat([ch3_output, pool_output], axis=-1)\n",
    "\n",
    "    \n",
    "def inception(input_data, num_classes=10, wd=0, update_collection=None, beta=1., reuse=None, training=False):\n",
    "    \"\"\"Mini-inception architecture (note that we do batch norm in absence of spectral norm)\"\"\"\n",
    "    \n",
    "    snconv_kwargs = dict(spectral_norm=False, reuse=reuse, training=training, bn=True)\n",
    "    input_data_reshaped = tf.reshape(input_data,[-1,28,28,1])\n",
    "    layer1 = tf.nn.relu(sn.conv2d(input_data_reshaped, [3, 3, 1, 96], scope_name='conv1', **snconv_kwargs))\n",
    "    layer2 = incept(layer1, 96, 32, 32, scope_name='incept2', **snconv_kwargs)\n",
    "    layer3 = incept(layer2, 32+32, 32, 48, scope_name='incept3', **snconv_kwargs)\n",
    "    layer4 = downsample(layer3, 32+48, 80, scope_name='downsamp4', **snconv_kwargs)\n",
    "    layer5 = incept(layer4, 80+32+48, 112, 48, scope_name='incept5', **snconv_kwargs)\n",
    "    layer6 = incept(layer5, 112+48, 96, 64, scope_name='incept6', **snconv_kwargs)\n",
    "    layer7 = incept(layer6, 96+64, 80, 80, scope_name='incept7', **snconv_kwargs)\n",
    "    layer8 = incept(layer7, 80+80, 48, 96, scope_name='incept8', **snconv_kwargs)\n",
    "    layer9 = downsample(layer8, 48+96, 96, scope_name='downsamp9', **snconv_kwargs)\n",
    "    layer10 = incept(layer9, 96+48+96, 176, 160, scope_name='incept10', **snconv_kwargs)\n",
    "    layer11 = incept(layer10, 176+160, 176, 160, scope_name='incept11', **snconv_kwargs)\n",
    "    layer12 = tf.nn.pool(layer11, window_shape=[7, 7], pooling_type='AVG', \n",
    "                         padding='SAME', strides=[1, 1], name='mean_pool12')\n",
    "    \n",
    "    fc = sn.linear(layer12, num_classes, scope_name='fc', spectral_norm=False, xavier=True, reuse=reuse)\n",
    "        \n",
    "    return fc\n",
    "\n",
    "def alexnet(input_data, num_classes=10, wd=0, update_collection=None, beta=1., reuse=None, training=False):\n",
    "    \"\"\"AlexNet architecture\n",
    "        two [convolution 5x5 -> max-pool 3x3 -> local-response-normalization] modules \n",
    "        followed by two fully connected layers with 384 and 192 hidden units, respectively. \n",
    "        Finally a NUM_CLASSES-way linear layer is used for prediction\n",
    "    \"\"\"\n",
    "    input_data_reshaped = tf.reshape(input_data,[-1,28,28,1])\n",
    "    conv = sn.conv2d(input_data_reshaped, [5, 5, 1, 96], scope_name='conv1', spectral_norm=False, reuse=reuse)\n",
    "    conv1 = tf.nn.relu(conv, name='conv1_relu')\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                           padding='VALID', name='pool1')\n",
    "    norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "    #norm1= pool1\n",
    "    \n",
    "    conv = sn.conv2d(norm1, [5, 5, 96, 256], scope_name='conv2', spectral_norm=False, reuse=reuse)\n",
    "    conv2 = tf.nn.relu(conv, name='conv2_relu')\n",
    "    pool2 = tf.nn.max_pool(conv2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                           padding='VALID', name='pool2')\n",
    "    norm2 = tf.nn.lrn(pool2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "    #norm2 = pool2\n",
    "    \n",
    "    reshape = tf.reshape(norm2, [-1, 6*6*256])\n",
    "    lin = sn.linear(reshape, 384, scope_name='linear1', spectral_norm=False, reuse=reuse)\n",
    "    lin1 = tf.nn.relu(lin, name='linear1_relu')\n",
    "\n",
    "    lin = sn.linear(lin1, 192, scope_name='linear2', spectral_norm=False, reuse=reuse)\n",
    "    lin2 = tf.nn.relu(lin, name='linear2_relu')\n",
    "\n",
    "    fc = sn.linear(lin2, num_classes, scope_name='fc', spectral_norm=False, reuse=reuse)\n",
    "        \n",
    "    return fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gridsan/farnia/FedAdv/tflib/sn.py:35: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gridsan/farnia/FedAdv/tflib/sn.py:37: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "stepsize_adv_delta = 0.02\n",
    "stepsize_adv_gamma = 0.001\n",
    "adv_lambda = 0.1\n",
    "LAMBDA_0 = 4.0\n",
    "LAMBDA_1 = 100.0\n",
    "norm_cons = 2.5\n",
    "norm_train = 2.5\n",
    "adv_stepsize = 0.05\n",
    "norm_max_0 = 1.0\n",
    "norm_max_1 = 5.0\n",
    "\n",
    "real_data = tf.placeholder(tf.float32, shape=[BATCH_SIZE, INPUT_DIM])\n",
    "label = tf.placeholder(tf.int64, shape=[BATCH_SIZE])\n",
    "delta = tf.placeholder(tf.float32, shape=[1, INPUT_DIM])\n",
    "Gamma = tf.placeholder(tf.float32, shape=[INPUT_DIM, INPUT_DIM])\n",
    "data_perturbed = tf.matmul(real_data,Gamma) + delta\n",
    "\n",
    "if neural_net==\"AlexNet\":\n",
    "    NN_out_perturbed = alexnet(data_perturbed )\n",
    "elif neural_net==\"Inception\":\n",
    "    NN_out_perturbed = inception(data_perturbed )\n",
    "    \n",
    "train_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(NN_out_perturbed,axis=1),label),dtype=tf.float32))\n",
    "\n",
    "train_loss= tf.reduce_mean( tf.log(tf.reduce_sum(tf.exp(NN_out_perturbed),reduction_indices=[1]))\n",
    "                           - tf.diag_part(tf.gather(NN_out_perturbed,label,axis=1)))\n",
    "attack_power = tf.reduce_sum(delta**2)\n",
    "Gamma_power = tf.reduce_sum( (tf.eye(num_rows=INPUT_DIM,dtype=tf.float32)-Gamma)**2 )\n",
    "train_loss = BATCH_SIZE*train_loss - LAMBDA_0 * attack_power - LAMBDA_1 * Gamma_power\n",
    "\n",
    "gradients_delta = tf.gradients(train_loss,delta)[0]\n",
    "delta_update = delta + stepsize_adv_delta*gradients_delta \n",
    "gradients_gamma = tf.gradients(train_loss,Gamma)[0]\n",
    "Gamma_update = Gamma + stepsize_adv_gamma*gradients_gamma\n",
    "\n",
    "\n",
    "real_data_agg = tf.placeholder(tf.float32, shape=[BATCH_SIZE*nodes, INPUT_DIM])\n",
    "label_agg = tf.placeholder(tf.int64, shape=[BATCH_SIZE*nodes])\n",
    "\n",
    "if neural_net==\"AlexNet\":\n",
    "    NN_out_agg = alexnet(real_data_agg,reuse=True)\n",
    "elif neural_net==\"Inception\":\n",
    "    NN_out_agg = inception(real_data_agg,reuse=True)\n",
    "    \n",
    "\n",
    "train_loss_agg= tf.reduce_mean( tf.log(tf.reduce_sum(tf.exp(NN_out_agg),reduction_indices=[1]))\n",
    "                           - tf.diag_part(tf.gather(NN_out_agg,label_agg,axis=1)))\n",
    "train_acc_agg = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(NN_out_agg,axis=1),label_agg),dtype=tf.float32))\n",
    "\n",
    "\n",
    "valid_data = tf.placeholder(tf.float32, shape=[TEST_BATCH_SIZE, INPUT_DIM])\n",
    "delta_valid = tf.placeholder(tf.float32, shape=[1, INPUT_DIM])\n",
    "Gamma_valid = tf.placeholder(tf.float32, shape=[INPUT_DIM, INPUT_DIM])\n",
    "valid_label = tf.placeholder(tf.int64, shape=[TEST_BATCH_SIZE])\n",
    "\n",
    "if neural_net==\"AlexNet\":\n",
    "    valid_NN_out = alexnet(tf.matmul(valid_data,Gamma_valid) + delta_valid,reuse=True)\n",
    "elif neural_net==\"Inception\":\n",
    "    valid_NN_out = inception(tf.matmul(valid_data,Gamma_valid) + delta_valid,reuse=True)\n",
    "\n",
    "valid_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(valid_NN_out,axis=1),valid_label),dtype=tf.float32))\n",
    "valid_loss= tf.reduce_mean( tf.log(tf.reduce_sum(tf.exp(valid_NN_out),reduction_indices=[1]))\n",
    "                          - tf.diag_part(tf.gather(valid_NN_out,valid_label,axis=1))  )\n",
    "             \n",
    "delta_valid_update = delta_valid + adv_stepsize * tf.gradients(valid_loss,delta_valid)[0]\n",
    "delta_valid_update = delta_valid_update / tf.transpose([tf.maximum(tf.norm(delta_valid_update)/norm_max_0,1.)]) \n",
    "Gamma_valid_update = Gamma_valid + adv_stepsize * tf.gradients(valid_loss,Gamma_valid)[0]\n",
    "Gamma_valid_update = tf.eye(num_rows=INPUT_DIM,dtype=tf.float32) + ((Gamma_valid_update-tf.eye(num_rows=INPUT_DIM,dtype=tf.float32))\n",
    "                       / tf.transpose([tf.maximum(tf.norm(Gamma_valid_update-tf.eye(num_rows=INPUT_DIM,dtype=tf.float32))/norm_max_1,1.)])  )\n",
    "\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "nn_params = tf.trainable_variables()\n",
    "Classifier_train_op = tf.train.GradientDescentOptimizer(\n",
    "        learning_rate=1e-3\n",
    "    ).minimize(train_loss_agg, var_list=nn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen, dev_gen, test_gen = lib.mnist_fed.load(BATCH_SIZE, TEST_BATCH_SIZE, k= nodes)\n",
    "def inf_train_gen():\n",
    "    while True:\n",
    "        for elements in train_gen():\n",
    "            for (images,targets) in elements:\n",
    "                yield images,targets\n",
    "            \n",
    "def inf_test_gen():\n",
    "    while True:\n",
    "        for elements in test_gen():\n",
    "            for (images,targets) in elements:\n",
    "                yield images,targets\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_loss_arr = []\n",
    "train_acc_arr= []\n",
    "train_loss_perturbed_arr = []\n",
    "train_acc_perturbed_arr= []\n",
    "valid_acc_arr = []\n",
    "valid_acc_perturbed_arr = []\n",
    "\n",
    "np.random.seed(1)\n",
    "perturbation_add_train = noise_std*np.random.normal(size=[nodes,INPUT_DIM])\n",
    "matrix_mult_train = (noise_std/np.sqrt(INPUT_DIM))*np.random.normal(size=[nodes,INPUT_DIM,INPUT_DIM])\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                          gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.45))) as session:\n",
    "\n",
    "\n",
    "    session.run(tf.initialize_all_variables())\n",
    "    gen = inf_train_gen()\n",
    "    gen_test = inf_test_gen()\n",
    "    delta_np=np.array([np.random.normal(size=[1,INPUT_DIM ])/(1e12)]*nodes)\n",
    "    gamma_np=np.array([np.eye(INPUT_DIM,dtype=np.float32)]*nodes)\n",
    "    _data_agg = np.zeros([BATCH_SIZE*nodes,INPUT_DIM],dtype=np.float32)\n",
    "    _data_perturbed_agg = np.zeros([BATCH_SIZE*nodes,INPUT_DIM],dtype=np.float32)\n",
    "    _labels_agg = np.zeros([BATCH_SIZE*nodes],dtype=np.int64)\n",
    "    for iteration in range(ITERS):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for k in range(nodes):    \n",
    "            \n",
    "            data_inf = next(gen)\n",
    "            _data = data_inf[0]\n",
    "            _data_agg[k*BATCH_SIZE:(k+1)*BATCH_SIZE,:] = _data\n",
    "            _data_agg[k*BATCH_SIZE:(k+1)*BATCH_SIZE,:] += np.matmul(_data_agg[k*BATCH_SIZE:(k+1)*BATCH_SIZE,:],\n",
    "                                                                   np.squeeze(matrix_mult_train[k,:,:]))\n",
    "            _data_agg[k*BATCH_SIZE:(k+1)*BATCH_SIZE,:] +=  perturbation_add_train[k,:]\n",
    "            _labels = data_inf[1]\n",
    "            _labels_agg[k*BATCH_SIZE:(k+1)*BATCH_SIZE] = _labels\n",
    "            \n",
    "            for _ in range(maximize_iters):\n",
    "                delta_np[k],gamma_np[k],_data_perturbed = session.run([delta_update,Gamma_update,data_perturbed],\n",
    "                                                                      feed_dict={real_data: _data,label: _labels\n",
    "                                                              ,delta: delta_np[k], Gamma:gamma_np[k]})\n",
    "                \n",
    "            _data_perturbed_agg[k*BATCH_SIZE:(k+1)*BATCH_SIZE,:] = _data_perturbed\n",
    "                #_data_agg[k*BATCH_SIZE:(k+1)*BATCH_SIZE,:]\n",
    "            \n",
    "        _,_train_loss_perturbed,_train_acc_perturbed = session.run([Classifier_train_op,train_loss_agg,train_acc_agg],\n",
    "                                                              feed_dict={real_data_agg: _data_perturbed_agg,\n",
    "                                                                         label_agg: _labels_agg})                             \n",
    "        _train_loss,_train_acc = session.run([train_loss_agg,train_acc_agg],\n",
    "                                             feed_dict={real_data_agg: _data_agg,\n",
    "                                                      label_agg: _labels_agg})                             \n",
    "    \n",
    "        lib.plot.plot(address+'/train_loss_perturbed', _train_loss_perturbed)\n",
    "        lib.plot.plot(address+'/train_acc_perturbed', _train_acc_perturbed)\n",
    "        lib.plot.plot(address+'/train_loss', _train_loss)\n",
    "        lib.plot.plot(address+'/train_acc', _train_acc)\n",
    "        train_loss_perturbed_arr.append(_train_loss_perturbed)\n",
    "        train_acc_perturbed_arr.append(_train_acc_perturbed)\n",
    "        train_loss_arr.append(_train_loss)\n",
    "        train_acc_arr.append(_train_acc)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Write logs every 500 iters\n",
    "        \n",
    "        if iteration % 1000 == 0:\n",
    "            test_data_inf = next(gen_test)\n",
    "            _data_valid = test_data_inf[0]\n",
    "            _labels_valid = test_data_inf[1]      \n",
    "\n",
    "            delta_valid_np=np.random.normal(size=[1,INPUT_DIM ])/(1e12)\n",
    "            gamma_valid_np=np.eye(INPUT_DIM,dtype=np.float32)\n",
    "            \n",
    "            _valid_acc  = session.run(valid_acc, feed_dict={valid_data: _data_valid,valid_label:_labels_valid,\n",
    "                                                           Gamma_valid: gamma_valid_np, delta_valid: delta_valid_np})      \n",
    "            for _ in range(test_iters):\n",
    "                 gamma_valid_np,delta_valid_np= session.run([Gamma_valid_update,delta_valid_update],\n",
    "                                                            feed_dict={valid_data: _data_valid,valid_label:_labels_valid,\n",
    "                                                           Gamma_valid: gamma_valid_np, delta_valid: delta_valid_np}) \n",
    "            \n",
    "            _valid_acc_perturbed  = session.run(valid_acc, feed_dict={valid_data: _data_valid,valid_label:_labels_valid,\n",
    "                                                           Gamma_valid: gamma_valid_np, delta_valid: delta_valid_np})      \n",
    "\n",
    "            valid_acc_arr.append(_valid_acc)\n",
    "            valid_acc_perturbed_arr.append(_valid_acc_perturbed)\n",
    "            \n",
    "            lib.plot.plot(address+'/valid_acc_nonadversarial', _valid_acc)\n",
    "            lib.plot.plot(address+'/valid_acc_perturbed', _valid_acc_perturbed)\n",
    "            \n",
    "            \n",
    "            np.save(address+'/train_loss_arr',train_loss_arr)\n",
    "            np.save(address+'/train_acc_arr',train_acc_arr)\n",
    "            np.save(address+'/train_loss_perturbed_arr',train_loss_perturbed_arr)\n",
    "            np.save(address+'/train_acc_perturbed_arr',train_acc_perturbed_arr)\n",
    "            np.save(address+'/valid_acc_arr',valid_acc_arr)\n",
    "            np.save(address+'/valid_acc_perturbed_arr',valid_acc_perturbed_arr)\n",
    "            \n",
    "        if iteration % 1000 == 0 and iteration>0:\n",
    "            saver.save(session, address+\"/model_\"+str(iteration))\n",
    "            \n",
    "        if iteration % 50 == 0 or iteration<10:\n",
    "            lib.plot.flush()\n",
    "\n",
    "        lib.plot.tick()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
